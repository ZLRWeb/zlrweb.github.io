---
slug: tech-summary-2025-06-30
title: TechSummary 2025-06-30：本地大型語言模型 (LLMs) 在工具調用的實務評測
authors: openai
tags: [local-llm, tool-calling, AI-evaluation, model-performance, Docker, NLP]

---

# TechSummary 2025-06-30：本地大型語言模型 (LLMs) 在工具調用的實務評測

## 🎯 研究背景與目的
這篇文章探討在建立 GenAI 應用時，選擇何種本地模型用於工具調用最合適。透過 Docker Model Runner 進行實驗，分析不同模型在工具調用上的表現，範例是模擬一個 AI 購物客服，測試模型呼叫工具（例如加入商品、搜尋商品）的能力。

---

## 🧪 初步手動測試的經驗
- 建立 chat2cart（購物助手）進行人工測試，模型需在自然對話中適時呼叫工具。
- OpenAI GPT-4 表現良好，能自然呼叫工具，避免不必要的呼叫。
- 本地模型（來自 Berkeley Function-Calling Leaderboard 的 xLAM-2-8b-fc-r 和 watt-tool-8B）卻遇到問題：
  - 過早呼叫工具（如問候時呼叫工具）
  - 工具選擇錯誤（搜尋時實際加入、刪除時誤搜尋）
  - 引數錯誤（參數缺失或錯誤）
  - 忽略工具回應
- 手動測試不足以支持大量測試，效率低且結果不穩定。

---

## 🚀 建立可擴展的測試框架
- 開發 `model-test` 測試工具，快速建立測試案例，模擬實世界情境。
- 支援多模型、多測試案例（包含不同的呼叫序列），自訂測試套件。
- 測試內容：
  - 單步操作（例如：打招呼）
  - 多步推理（工具連鎖）
  - 模擬代理人（最多 5 回合）決策流程。

### 測試流程範例
```json
{
  "prompt": "Add iPhone to cart",
  "expected_tools_variants": [
    {
      "name": "direct_add",
      "tools": [{ "name": "add_to_cart", "arguments": { "product_name": "iPhone" } }]
    },
    {
      "name": "search_then_add",
      "tools": [
        { "name": "search_products", "arguments": { "query": "iPhone" } },
        { "name": "add_to_cart", "arguments": { "product_name": "iPhone 15" } }
      ]
    }
  ]
}
```
- 測試接受多種正確方案（非嚴格匹配），更接近實際應用。

---

## 🔍 量測指標與評估方法
- 核心度量：
  - **工具觸發**：模型是否識別需要工具。
  - **工具選擇**：選對工具且用法正確。
  - **引數準確性**：工具呼叫的引數是否正確。
- 評分方式：
  - 使用 F1 分數（精確率與召回率的調和平均）評估工具呼叫品質。
  - 另外監測平均反應時間 (latency)，用於速度評估。

---

## 🏆 模型表現結果
- 於 21 款模型下，進行 3570 個測試案例（210 批次測試）：
  - **最佳：OpenAI GPT-4**，F1 0.974，平均反應約 5 秒。
  - **本地模型中，Qwen 3 (14B)** 表現最接近 GPT-4，F1 0.971，但平均耗時約 142 秒。
  - 8B 版本 Qwen 3 也取得 0.933，反應較快 (~84 秒)，為效能與速度的平衡選擇。
  - 雲端模型 Claude 3 Haiku 亦表現良好（F1 0.933），延遲僅約 3.56 秒。

- 不理想者：
  - Watt 8B（量化模型）反應差，F1 僅 0.484。
  - LLaMA 8B (XLam) F1 0.570，經常漏掉正確工具。

---

## ⚖️ 量化模型的影響
- 實驗顯示量化與非量化模型在工具呼叫行為上沒顯著差異，暗示量化可以在降低資源消耗的同時保持性能。

---

## 🎯 製作建議
- **高精度**：Qwen 3 (14B)，或速度優先的 Qwen 3 (8B)。 
- **平均速度與準確**：Qwen 2.5，速度適中，工具選擇能力良好。
- **資源有限**：LLaMA 3 Groq 7B，負擔較低但性能較弱。

---

## 🌟 重點與心得
- Qwen 系列特別是 14B 和 8B 在開源模組中，工具調用能力領先。
- 高階模型（如 GPT-4）雖然表現優異，但考量實務中成本、反應時間，較適合作為基準而非一直使用的模型。
- 這次的評測框架幫助縮小範圍，從「不知道用哪個模型」到「有三個效果良好的選擇」，提升模型選擇的信心與效率。
- 建議業界利用 `model-test` 自行測試判斷，避免盲目跟風。

---

## 🔗 參考資源
- Docker Model Runner 架構詳解
- 模型分發規範故事
- 快速入門指南
- 官方文件與社群支援
---

### 🌱 我的看法
這篇文章展現了善用自動化測試框架來分析模型性能的重要性。隨著 LLM 技術的快速發展，單純追求更大模型已不再是唯一路徑，精細評估模型在特定任務中的表現，尤其是工具調用，能大幅提升模型的實用性與穩定性。建議開發者掌握這種測試思維，根據實際需求選擇合適的模型，才能真正落實 AI 應用的效果。

---

## 📚 參考來源
- [Docker官方博文](https://www.docker.com/blog/local-llm-tool-calling-a-practical-evaluation/)
